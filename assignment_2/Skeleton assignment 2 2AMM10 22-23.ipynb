{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d32f8d18",
   "metadata": {},
   "source": [
    "# Group Details\n",
    "\n",
    "## Group Name:\n",
    "\n",
    "### Student 1:\n",
    "\n",
    "### Student 2:\n",
    "\n",
    "### Student 3:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "faec2056",
   "metadata": {},
   "source": [
    "# Loading Data and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d0580a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0756591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(filename, task):\n",
    "    datapoint = np.load(filename)\n",
    "    if task == 'task 1':\n",
    "        initial_state = datapoint['initial_state']\n",
    "        terminal_state = datapoint['terminal_state']\n",
    "        return initial_state, terminal_state\n",
    "    elif task == 'task 2' or task == 'task 3':\n",
    "        whole_trajectory = datapoint['trajectory']\n",
    "        # change shape: (num_bodies, attributes, time) ->  num_bodies, time, attributes\n",
    "        whole_trajectory = np.swapaxes(whole_trajectory, 1, 2)\n",
    "        initial_state = whole_trajectory[:, 0]\n",
    "        target = whole_trajectory[:, 1:, 1:]  # drop the first timepoint (second dim) and mass (last dim) for the prediction task\n",
    "        return initial_state, target\n",
    "    else:\n",
    "        raise NotImplementedError(\"'task' argument should be 'task 1', 'task 2' or 'task 3'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb77a4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of initial state (model input): (8, 5)\n",
      "shape of terminal state (to be predicted by model): (8, 2)\n",
      "The initial x-coordinate of the body with index 2 in this trajectory was -5.159721083543527\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell gives an example of loading a datapoint with numpy for task 1.\n",
    "\n",
    "The arrays returned by the function are structures as follows:\n",
    "initial_state: shape (n_bodies, [mass, x, y, v_x, v_y])\n",
    "terminal_state: shape (n_bodies, [x, y])\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "example = load_array('data/task 1/train/trajectory_0.npz', task='task 1')\n",
    "\n",
    "initial_state, terminal_state = example\n",
    "print(f'shape of initial state (model input): {initial_state.shape}')\n",
    "print(f'shape of terminal state (to be predicted by model): {terminal_state.shape}')\n",
    "\n",
    "body_idx = 2\n",
    "print(f'The initial x-coordinate of the body with index {body_idx} in this trajectory was {initial_state[body_idx, 1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953ecece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of initial state (model input): (5, 5)\n",
      "shape of terminal state (to be predicted by model): (5, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "example = load_array('data/task 1/train/trajectory_1.npz', task='task 1')\n",
    "\n",
    "initial_state, terminal_state = example\n",
    "print(f'shape of initial state (model input): {initial_state.shape}')\n",
    "print(f'shape of terminal state (to be predicted by model): {terminal_state.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c3ea4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of initial state (model input): (8, 5)\n",
      "shape of terminal state (to be predicted by model): (8, 49, 4)\n",
      "The y-coordinate of the body with index 2 at time with index 30 in remaining_trajectory was -0.3861544940435097\n",
      "the shape of the input of a test data example is (8, 5)\n",
      "the shape of the target of a test data example is (8, 49, 4)\n",
      "values of the test data example at time 30:\n",
      " [[-1.11611543  3.21149953         nan         nan]\n",
      " [-0.2865083   4.30801877         nan         nan]\n",
      " [ 1.07701594 -8.12529269         nan         nan]\n",
      " [-0.92053478  3.13709551         nan         nan]\n",
      " [-3.96308297 -4.27733589         nan         nan]\n",
      " [ 2.33945401 -8.67733599         nan         nan]\n",
      " [-4.83949085  3.67854952         nan         nan]\n",
      " [ 0.31080159 -9.74720071         nan         nan]]\n",
      "note: velocity values are unobserved (NaNs) in the test data!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell gives an example of loading a datapoint with numpy for task 2 / 3.\n",
    "\n",
    "The arrays returned by the function are structures as follows:\n",
    "initial_state: shape (n_bodies, [mass, x, y, v_x, v_y])\n",
    "remaining_trajectory: shape (n_bodies, time, [x, y, v_x, v_y])\n",
    "\n",
    "Note that for this task, you are asked to evaluate performance only with regard to the predictions of the positions (x and y).\n",
    "If you use the velocity of the remaining trajectory for training,\n",
    "this use should be purely auxiliary for the goal of predicting the positions [x,y] over time. \n",
    "While testing performance of your model on the test set, you do not have access to v_x and v_y of the remaining trajectory.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "example = load_array('data/task 2_3/train/trajectory_0.npz', task='task 2')\n",
    "\n",
    "initial_state, remaining_trajectory = example\n",
    "print(f'shape of initial state (model input): {initial_state.shape}')\n",
    "print(f'shape of terminal state (to be predicted by model): {remaining_trajectory.shape}')\n",
    "\n",
    "body_idx = 2\n",
    "time_idx = 30\n",
    "print(f'The y-coordinate of the body with index {body_idx} at time with index {time_idx} in remaining_trajectory was {remaining_trajectory[body_idx, time_idx, 1]}')\n",
    "\n",
    "test_example = load_array('data/task 2_3/test/trajectory_900.npz', task='task 3')\n",
    "test_initial_state, test_remaining_trajectory = test_example\n",
    "print(f'the shape of the input of a test data example is {test_initial_state.shape}')\n",
    "print(f'the shape of the target of a test data example is {test_remaining_trajectory.shape}')\n",
    "print(f'values of the test data example at time {time_idx}:\\n {test_remaining_trajectory[:, time_idx]}')\n",
    "print('note: velocity values are unobserved (NaNs) in the test data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10a3438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9106543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" #Read data from multiple files\\ninitial_state_train=[]\\nterminal_state_train=[]\\nfor i in range(900):\\n    data =load_array('data/task 1/train/trajectory_'+str(i)+'.npz', task='task 1')\\n    initial_state_train.append(data[0])\\n    terminal_state_train.append(data[1]) \""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" #Read data from multiple files\n",
    "initial_state_train=[]\n",
    "terminal_state_train=[]\n",
    "for i in range(900):\n",
    "    data =load_array('data/task 1/train/trajectory_'+str(i)+'.npz', task='task 1')\n",
    "    initial_state_train.append(data[0])\n",
    "    terminal_state_train.append(data[1]) \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2b85cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For each intial state, measure the relative distance to the other bodies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bf5e56c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' max_length=0\\nfor i in range(900):\\n    if initial_state_train[i].shape[0]>max_length:\\n        max_length=initial_state_train[i].shape[0] '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the max length of the data\n",
    "\"\"\" max_length=0\n",
    "for i in range(900):\n",
    "    if initial_state_train[i].shape[0]>max_length:\n",
    "        max_length=initial_state_train[i].shape[0] \"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "059b633c",
   "metadata": {},
   "source": [
    "# Data Handling and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0490820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder):\n",
    "    file_list = os.listdir(folder)\n",
    "\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(folder, file_name)\n",
    "        input, target = load_array(file_path, 'task 1')\n",
    "        inputs.append(torch.from_numpy(input))\n",
    "        targets.append(torch.from_numpy(target))\n",
    "\n",
    "    return inputs, targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6479019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_data('data/task 1/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd539c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9effd8ab",
   "metadata": {},
   "source": [
    "0-Mass\n",
    "1-x\n",
    "2-y\n",
    "3-v_x\n",
    "4-v_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a834a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the pairwise combination of the  and 4th column of X_train[0]\n",
    "\n",
    "pairwise_vel= [(X_train[0][i][3],X_train[0][j][3]) for i in range(X_train[0].shape[0] ) for j in range(X_train[0].shape[0] ) if i!=j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e226169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split pairwise_vel into 8 lists with 7 elements each\n",
    "pairwise_vel_split = [pairwise_vel[i:i + 7] for i in range(0, len(pairwise_vel), 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ed8ddb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0969, -0.5787], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(pairwise_vel_split[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e40752da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairwise_velocities(x):\n",
    "    pairwise_velocities = []\n",
    "    for i in range(len(x)):\n",
    "        pairwise_velocities= [(x[i][j][3],x[i][k][3]) for j in range(x[i].shape[0] ) for k in range(x[i].shape[0] ) if j!=k]\n",
    "    return pairwise_velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dac58511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_array(data):\n",
    "    # Pad the array with zeros if necessary to have 9 rows\n",
    "    padded_input_data  = np.pad(data[0], ((0, 9 - data[0].shape[0]), (0, 0)), mode='constant')\n",
    "    padded_target_data = np.pad(data[1], ((0, 9 - data[1].shape[0]), (0, 0)), mode='constant') \n",
    "    return padded_input_data, padded_target_data\n",
    "\n",
    "def create_mask(data,padded_input_data):\n",
    "    # Create a boolean mask array indicating the padded rows\n",
    "    mask = np.ones_like(padded_input_data, dtype=bool)\n",
    "    mask[data[0].shape[0]:] = False\n",
    "    return mask\n",
    "\n",
    "def pair_values(data):\n",
    "    # Pair up the values of the same columns from every row with every other row\n",
    "    num_rows, num_cols = data.shape\n",
    "    padded_input_data = np.zeros((num_rows, num_rows, num_cols, 2))\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        pair_idx = 0\n",
    "        for j in range(num_rows):\n",
    "            \n",
    "            padded_input_data[i, pair_idx, :, 0] = data[i]\n",
    "            padded_input_data[i, pair_idx, :, 1] = data[j]\n",
    "            pair_idx += 1\n",
    "\n",
    "    return padded_input_data\n",
    "\n",
    "def get_euclidean_distance(x):\n",
    "    euclidean_distances = np.zeros((len(x), len(x)))\n",
    "    for i in range(len(x)):\n",
    "        source_x, source_y= x[i][1], x[i][2]\n",
    "        #euclidean_distance=[]\n",
    "        for j in range(len(x)):\n",
    "            target_x, target_y= x[j][1], x[j][2]\n",
    "            euclidean_distances[i][j]=np.sqrt((source_x-target_x)**2+(source_y-target_y)**2)\n",
    "            #euclidean_distance.append(np.sqrt((source_x-target_x)**2+(source_y-target_y)**2))\n",
    "        #euclidean_distances.append(euclidean_distance)\n",
    "\n",
    "    return euclidean_distances #list of lists\n",
    "\n",
    "def target_difference(target_x, source_x):\n",
    "    # target x and source x are of shape (9, 2)\n",
    "    #Subtract the x and y coordinates of the target from the source\n",
    "    return target_x - source_x\n",
    "\n",
    "\n",
    "def process_file(file_path):\n",
    "    # Main function to process a single file\n",
    "    data = load_array(file_path, 'task 1')\n",
    "    padded_input_data, padded_target_data = pad_array(data)\n",
    "    mask = create_mask(data,padded_input_data)\n",
    "    input_data = pair_values(padded_input_data)\n",
    "    euclidean_distance = get_euclidean_distance(padded_input_data)\n",
    "    target_data= target_difference(padded_target_data, padded_input_data[:,1:3])\n",
    "\n",
    "    return input_data, target_data, padded_input_data, mask, euclidean_distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6073689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset_2(Dataset):\n",
    "    def __init__(self, folder):\n",
    "        self.folder = folder\n",
    "        self.file_list = os.listdir(folder)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def _read_file(self, file_path):\n",
    "        input_data, target_data,data, mask, distances= process_file(file_path)\n",
    "        return input_data, target_data,data, mask, distances\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file_path = os.path.join(self.folder, self.file_list[index])\n",
    "        # Read and preprocess the data from the file\n",
    "        input_data, target_data,data, mask, distance = self._read_file(file_path)\n",
    "        # Return the preprocessed data\n",
    "        return input_data, target_data,data, mask, distance\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18b2874d",
   "metadata": {},
   "source": [
    "# Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66774050",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DeepSet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "\n",
    "        )\n",
    "        self.aggregator = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim[0]*output_dim[1])\n",
    "        )\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x, mask, distance):\n",
    "        batch_size = x.shape[0]\n",
    "        encoded = self.encoder(x)\n",
    "        aggregated = torch.mean(encoded, dim=1)\n",
    "        aggregated = self.aggregator(aggregated)\n",
    "        output = self.decoder(aggregated)\n",
    "        output = output.view(batch_size, self.output_dim[0], self.output_dim[1])\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "17dd4ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DeepSetRNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim,activation= nn.LeakyReLU()):\n",
    "        super(DeepSetRNN, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            activation,\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            activation,\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            activation,\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            \n",
    "        )\n",
    "        self.aggregator = nn.GRU(hidden_dim, hidden_dim, batch_first=True) # Replace with GRU\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            activation,\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            activation,\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            activation,\n",
    "            nn.Linear(hidden_dim, output_dim[0] * output_dim[1])\n",
    "        )\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        encoded = self.encoder(x)\n",
    "        _, aggregated = self.aggregator(encoded)  # Use the RNN aggregator\n",
    "        aggregated = aggregated.squeeze(0)  # Remove the sequence dimension\n",
    "        output = self.decoder(aggregated)\n",
    "        output = output.view(batch_size, self.output_dim[0], self.output_dim[1])\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ba598378",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 5\n",
    "hidden_dim = 64\n",
    "output_dim = [9,2]  # Output x and y coordinates\n",
    "batch_size = 32\n",
    "num_epochs = 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44baf9a4",
   "metadata": {},
   "source": [
    "## NPE implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95154df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPEEncoder(nn.Module):\n",
    "    def __init__(self, hidden_units):\n",
    "        super(NPEEncoder, self).__init__()\n",
    "        self.pairwise_layer = nn.Linear(10, hidden_units, bias=False)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(hidden_units, 50, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50, bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x= x.to(torch.float32)\n",
    "        x = self.pairwise_layer(x)\n",
    "        x = self.feedforward(x)\n",
    "        return x\n",
    "\n",
    "class NPEDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NPEDecoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(55, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebe6c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NPEModel(nn.Module):\n",
    "    def __init__(self, hidden_units):\n",
    "        super(NPEModel, self).__init__()\n",
    "        self.encoder = NPEEncoder(hidden_units)\n",
    "        self.decoder = NPEDecoder()\n",
    "    \n",
    "    def forward(self, x,unpaired_data, mask, distance, neighborhood_threshold=5):\n",
    "        output=[]\n",
    "        for batch in range(x.size(0)):\n",
    "            batch_data= x[batch]\n",
    "            batch_output=[]\n",
    "            for body in range(batch_data.size(0)):\n",
    "                if torch.all(mask[batch][body]==1):\n",
    "                    focus_body= unpaired_data[batch][body]\n",
    "\n",
    "                    #get index where the distance is greater than 0 but less than threshold\n",
    "                    indices = np.where((distance[batch][body] > 0) & (distance[batch][body] < neighborhood_threshold))\n",
    "                    \n",
    "\n",
    "                    # Iterate through the row chunks\n",
    "                    neighbor_encodings = []\n",
    "                    for index in indices[0]:\n",
    "                        # Process each row\n",
    "                        chunk= x[batch][body][index]\n",
    "                        chunk= torch.flatten(chunk)\n",
    "                        chunk = chunk.squeeze()\n",
    "                        processed_row = self.encoder(chunk)\n",
    "                        neighbor_encodings.append(processed_row)\n",
    "                    \n",
    "                    # Take the sum of the neighbor encodings\n",
    "                    if neighbor_encodings:\n",
    "                        neighbor_encodings = torch.sum(torch.stack(neighbor_encodings), dim=0)\n",
    "                    else:\n",
    "                        neighbor_encodings = torch.zeros(50)\n",
    "                    # Concatenate the focus body encoding with the focus_body\n",
    "                    decoder_input = torch.cat((neighbor_encodings, focus_body), dim=0)\n",
    "                    decoder_input= decoder_input.to(torch.float32)\n",
    "                    # Decode the concatenated vector\n",
    "                    decoded = self.decoder(decoder_input)\n",
    "                    batch_output.append(decoded)\n",
    "                else:\n",
    "                    batch_output.append(torch.zeros(2))\n",
    "            output.append(batch_output)\n",
    "            \n",
    "\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dea70d73",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3af520ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DeepSet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[151], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m DeepSet(input_dim, hidden_dim, output_dim)\n\u001b[0;32m      3\u001b[0m \u001b[39m# Create the data loader\u001b[39;00m\n\u001b[0;32m      4\u001b[0m dataset \u001b[39m=\u001b[39m CustomDataset(data_dir)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DeepSet' is not defined"
     ]
    }
   ],
   "source": [
    "model = DeepSet(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Create the data loader\n",
    "dataset = CustomDataset(data_dir)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8bf46187",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = 'data/task 1/test'\n",
    "test_dataset = CustomDataset(test_data_dir)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6de0828c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DeepSetRNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m DeepSetRNN(input_dim, hidden_dim, output_dim)\n\u001b[0;32m      3\u001b[0m \u001b[39m# Create the data loader\u001b[39;00m\n\u001b[0;32m      4\u001b[0m dataset \u001b[39m=\u001b[39m CustomDataset(data_dir)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DeepSetRNN' is not defined"
     ]
    }
   ],
   "source": [
    "model = DeepSetRNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Create the data loader\n",
    "dataset = CustomDataset(data_dir)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1ee07941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 12.599453926086426, Test Loss: 14.747041029559664\n",
      "Epoch [2/50], Loss: 15.24623966217041, Test Loss: 14.212319937163151\n",
      "Epoch [3/50], Loss: 9.712862014770508, Test Loss: 15.662670619772591\n",
      "Epoch [4/50], Loss: 17.36570167541504, Test Loss: 14.065373895045274\n",
      "Epoch [5/50], Loss: 12.51584243774414, Test Loss: 14.187860914964032\n",
      "Epoch [6/50], Loss: 9.923476219177246, Test Loss: 12.975637932418723\n",
      "Epoch [7/50], Loss: 9.939854621887207, Test Loss: 12.88765626277024\n",
      "Epoch [8/50], Loss: 18.01913070678711, Test Loss: 12.952148250987916\n",
      "Epoch [9/50], Loss: 13.629838943481445, Test Loss: 13.140239588249504\n",
      "Epoch [10/50], Loss: 12.470887184143066, Test Loss: 13.359621736682953\n",
      "Epoch [11/50], Loss: 17.428478240966797, Test Loss: 13.773844002575249\n",
      "Epoch [12/50], Loss: 14.444316864013672, Test Loss: 12.438155121061193\n",
      "Epoch [13/50], Loss: 12.469012260437012, Test Loss: 12.827209521209065\n",
      "Epoch [14/50], Loss: 9.219258308410645, Test Loss: 11.945247133675291\n",
      "Epoch [15/50], Loss: 14.09851360321045, Test Loss: 13.550298671111719\n",
      "Epoch [16/50], Loss: 10.99209213256836, Test Loss: 12.449333585435197\n",
      "Epoch [17/50], Loss: 13.73572063446045, Test Loss: 12.822251834704\n",
      "Epoch [18/50], Loss: 10.74199390411377, Test Loss: 12.359480243095373\n",
      "Epoch [19/50], Loss: 11.757152557373047, Test Loss: 12.457202218351242\n",
      "Epoch [20/50], Loss: 15.518583297729492, Test Loss: 13.570499976503868\n",
      "Epoch [21/50], Loss: 15.57252311706543, Test Loss: 12.035570966044466\n",
      "Epoch [22/50], Loss: 12.753523826599121, Test Loss: 12.260402227283713\n",
      "Epoch [23/50], Loss: 9.256596565246582, Test Loss: 12.179380715360265\n",
      "Epoch [24/50], Loss: 7.695811748504639, Test Loss: 12.475910009212964\n",
      "Epoch [25/50], Loss: 15.564640045166016, Test Loss: 13.209306767012993\n",
      "Epoch [26/50], Loss: 11.447229385375977, Test Loss: 11.373382806362102\n",
      "Epoch [27/50], Loss: 11.021330833435059, Test Loss: 10.81303730855511\n",
      "Epoch [28/50], Loss: 9.895041465759277, Test Loss: 11.751065412740978\n",
      "Epoch [29/50], Loss: 12.21560287475586, Test Loss: 11.638466286340488\n",
      "Epoch [30/50], Loss: 8.119098663330078, Test Loss: 11.197768495070578\n",
      "Epoch [31/50], Loss: 6.802387714385986, Test Loss: 11.478477416491149\n",
      "Epoch [32/50], Loss: 15.997669219970703, Test Loss: 10.742302250661602\n",
      "Epoch [33/50], Loss: 10.094141960144043, Test Loss: 12.49201617950867\n",
      "Epoch [34/50], Loss: 14.031668663024902, Test Loss: 12.4023924655717\n",
      "Epoch [35/50], Loss: 11.255959510803223, Test Loss: 11.666396347949116\n",
      "Epoch [36/50], Loss: 8.667329788208008, Test Loss: 12.110058203072528\n",
      "Epoch [37/50], Loss: 10.858882904052734, Test Loss: 10.71978403910599\n",
      "Epoch [38/50], Loss: 12.654803276062012, Test Loss: 11.584323045097856\n",
      "Epoch [39/50], Loss: 17.726566314697266, Test Loss: 11.597632838713267\n",
      "Epoch [40/50], Loss: 9.93181037902832, Test Loss: 11.538353911579376\n",
      "Epoch [41/50], Loss: 9.888575553894043, Test Loss: 11.43367270508477\n",
      "Epoch [42/50], Loss: 9.982362747192383, Test Loss: 11.772610089240917\n",
      "Epoch [43/50], Loss: 9.405238151550293, Test Loss: 11.982350939570985\n",
      "Epoch [44/50], Loss: 10.02822494506836, Test Loss: 11.515065707514156\n",
      "Epoch [45/50], Loss: 14.557525634765625, Test Loss: 10.879511390227956\n",
      "Epoch [46/50], Loss: 12.310267448425293, Test Loss: 11.475567920656893\n",
      "Epoch [47/50], Loss: 3.8158090114593506, Test Loss: 10.931720894479923\n",
      "Epoch [48/50], Loss: 8.91639518737793, Test Loss: 11.497006744998119\n",
      "Epoch [49/50], Loss: 6.615074157714844, Test Loss: 10.118651351605184\n",
      "Epoch [50/50], Loss: 13.242386817932129, Test Loss: 10.494498814154438\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for input_tensor, target_tensor in data_loader:\n",
    "        #set input and target to float\n",
    "        input = input_tensor.to('cpu').float()\n",
    "        target = target_tensor.to('cpu').float()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(input).to('cpu')\n",
    "        # Compute the loss\n",
    "        loss = criterion(output, target)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    test_loss=0.0\n",
    "    with torch.no_grad():\n",
    "        for input_tensor, target_tensor in test_data_loader:\n",
    "            input = input_tensor.to('cpu').float()\n",
    "            target = target_tensor.to('cpu').float()\n",
    "            output = model(input).to('cpu')\n",
    "            test_loss += criterion(output, target_tensor).item()\n",
    "    test_loss /= len(test_data_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}, Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e95af5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(batch))\n\u001b[0;32m      4\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> 5\u001b[0m output \u001b[39m=\u001b[39m model(batch)\n\u001b[0;32m      6\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, batch)\n\u001b[0;32m      7\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\DSAI-22-24\\TUe-Year1\\Q4\\Deep Learning\\tue-deeplearning\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[34], line 22\u001b[0m, in \u001b[0;36mDeepSet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 22\u001b[0m     encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m     23\u001b[0m     aggregated \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(encoded, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     24\u001b[0m     aggregated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregator(aggregated)\n",
      "File \u001b[1;32md:\\DSAI-22-24\\TUe-Year1\\Q4\\Deep Learning\\tue-deeplearning\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\DSAI-22-24\\TUe-Year1\\Q4\\Deep Learning\\tue-deeplearning\\.conda\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32md:\\DSAI-22-24\\TUe-Year1\\Q4\\Deep Learning\\tue-deeplearning\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\DSAI-22-24\\TUe-Year1\\Q4\\Deep Learning\\tue-deeplearning\\.conda\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch in data_loader:\n",
    "        print(type(batch))\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch)\n",
    "        loss = criterion(output, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d78cead1",
   "metadata": {},
   "source": [
    "## NPE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56a34e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/task 1/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ff06815",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset_2(data_dir)\n",
    "data_loader = DataLoader(dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = 'data/task 1/test'\n",
    "test_dataset = CustomDataset_2(test_data_dir)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7efb3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device( 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07e03ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_units = 25\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.99\n",
    "iterations = 50\n",
    "batch_size = 100\n",
    "\n",
    "# Create the model\n",
    "model = NPEModel(hidden_units)\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a760bea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 5.609353065490723, Test Loss: 4.0937652587890625\n",
      "Epoch [2/50], Loss: 5.379310131072998, Test Loss: 3.947829484939575\n",
      "Epoch [3/50], Loss: 5.259251117706299, Test Loss: 3.9337656497955322\n",
      "Epoch [4/50], Loss: 5.015397071838379, Test Loss: 3.7005913257598877\n",
      "Epoch [5/50], Loss: 4.872307300567627, Test Loss: 3.548375129699707\n",
      "Epoch [6/50], Loss: 4.806728839874268, Test Loss: 3.460207462310791\n",
      "Epoch [7/50], Loss: 4.856318473815918, Test Loss: 3.512981653213501\n",
      "Epoch [8/50], Loss: 4.50195837020874, Test Loss: 3.2242136001586914\n",
      "Epoch [9/50], Loss: 4.362918376922607, Test Loss: 3.1518945693969727\n",
      "Epoch [10/50], Loss: 4.400338172912598, Test Loss: 3.11505126953125\n",
      "Epoch [11/50], Loss: 4.286999702453613, Test Loss: 3.029372215270996\n",
      "Epoch [12/50], Loss: 4.098598957061768, Test Loss: 2.9292006492614746\n",
      "Epoch [13/50], Loss: 4.294901371002197, Test Loss: 2.9497263431549072\n",
      "Epoch [14/50], Loss: 4.181558609008789, Test Loss: 3.2847836017608643\n",
      "Epoch [15/50], Loss: 4.444196701049805, Test Loss: 3.11157488822937\n",
      "Epoch [16/50], Loss: 3.7960100173950195, Test Loss: 2.7395732402801514\n",
      "Epoch [17/50], Loss: 3.726452350616455, Test Loss: 2.6948623657226562\n",
      "Epoch [18/50], Loss: 3.691746950149536, Test Loss: 2.621168851852417\n",
      "Epoch [19/50], Loss: 3.9088993072509766, Test Loss: 2.7642788887023926\n",
      "Epoch [20/50], Loss: 3.589508295059204, Test Loss: 2.6021552085876465\n",
      "Epoch [21/50], Loss: 3.4915521144866943, Test Loss: 2.4941234588623047\n",
      "Epoch [22/50], Loss: 3.582843780517578, Test Loss: 2.527820587158203\n",
      "Epoch [23/50], Loss: 3.512314558029175, Test Loss: 2.566500663757324\n",
      "Epoch [24/50], Loss: 3.4872283935546875, Test Loss: 2.537623643875122\n",
      "Epoch [25/50], Loss: 3.422304391860962, Test Loss: 2.5294697284698486\n",
      "Epoch [26/50], Loss: 3.354020833969116, Test Loss: 2.3820230960845947\n",
      "Epoch [27/50], Loss: 3.3758773803710938, Test Loss: 2.5269694328308105\n",
      "Epoch [28/50], Loss: 3.47446346282959, Test Loss: 2.4873814582824707\n",
      "Epoch [29/50], Loss: 3.2045109272003174, Test Loss: 2.344320297241211\n",
      "Epoch [30/50], Loss: 3.1915338039398193, Test Loss: 2.2982568740844727\n",
      "Epoch [31/50], Loss: 3.159823179244995, Test Loss: 2.2894818782806396\n",
      "Epoch [32/50], Loss: 3.2242679595947266, Test Loss: 2.3872175216674805\n",
      "Epoch [33/50], Loss: 3.285522222518921, Test Loss: 2.36136794090271\n",
      "Epoch [34/50], Loss: 3.07236385345459, Test Loss: 2.258134126663208\n",
      "Epoch [35/50], Loss: 3.2051842212677, Test Loss: 2.3435208797454834\n",
      "Epoch [36/50], Loss: 3.1386497020721436, Test Loss: 2.343843698501587\n",
      "Epoch [37/50], Loss: 3.0432989597320557, Test Loss: 2.2388155460357666\n",
      "Epoch [38/50], Loss: 3.068902015686035, Test Loss: 2.2890682220458984\n",
      "Epoch [39/50], Loss: 3.1419684886932373, Test Loss: 2.351008653640747\n",
      "Epoch [40/50], Loss: 3.0059168338775635, Test Loss: 2.226322650909424\n",
      "Epoch [41/50], Loss: 2.936293125152588, Test Loss: 2.1901698112487793\n",
      "Epoch [42/50], Loss: 3.0055863857269287, Test Loss: 2.261239528656006\n",
      "Epoch [43/50], Loss: 3.0146732330322266, Test Loss: 2.2841694355010986\n",
      "Epoch [44/50], Loss: 2.938293218612671, Test Loss: 2.2133114337921143\n",
      "Epoch [45/50], Loss: 3.0685739517211914, Test Loss: 2.3116540908813477\n",
      "Epoch [46/50], Loss: 2.939953327178955, Test Loss: 2.2527542114257812\n",
      "Epoch [47/50], Loss: 2.93963885307312, Test Loss: 2.2184863090515137\n",
      "Epoch [48/50], Loss: 2.893454074859619, Test Loss: 2.231515407562256\n",
      "Epoch [49/50], Loss: 2.874160051345825, Test Loss: 2.185286283493042\n",
      "Epoch [50/50], Loss: 2.8782763481140137, Test Loss: 2.1893341541290283\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(iterations):\n",
    "    for input_data, target_data,unpaired_data, mask, distance in data_loader:\n",
    "    # Training steps\n",
    "        input_data = input_data.to(torch.float32)\n",
    "        unpaired_data = unpaired_data.to(torch.float32)\n",
    "        mask = mask.to(torch.float32)\n",
    "        distance = distance.to(torch.float32)\n",
    "        target_data=target_data.to(torch.float32)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_data, unpaired_data, mask, distance)\n",
    "        output = torch.stack([torch.stack(lst) for lst in output], dim=0)\n",
    "        \n",
    "        \n",
    "        loss = criterion(output, target_data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Learning rate decay\n",
    "        if (iteration + 1) % 10 == 0:\n",
    "            learning_rate *= learning_rate_decay\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate\n",
    "        \n",
    "\n",
    "        model.eval()\n",
    "        test_loss=0.0\n",
    "        with torch.no_grad():\n",
    "            for input_data, target_data,unpaired_data, mask, distance in test_data_loader:\n",
    "                input_data = input_data.to(torch.float32)\n",
    "                target_data = target_data.to(torch.float32)\n",
    "                unpaired_data = unpaired_data.to(torch.float32)\n",
    "                mask = mask.to(torch.float32)\n",
    "                distance = distance.to(torch.float32)\n",
    "                output = model(input_data, unpaired_data, mask, distance)\n",
    "                output = torch.stack([torch.stack(lst) for lst in output], dim=0)\n",
    "                test_loss += criterion(output, target_data).item()\n",
    "        test_loss /= len(test_data_loader)\n",
    "\n",
    "                \n",
    "        # Print loss for monitoring\n",
    "        #if (iteration + 1) % 10000 == 0:\n",
    "    print(f\"Epoch [{iteration+1}/{iterations}], Loss: {loss.item()}, Test Loss: {test_loss}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5fb3b29",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5fa1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2280031f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8240f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
